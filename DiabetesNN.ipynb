{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa0fb6b-b682-4698-800e-68631e34bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f2d23e5-8376-4df3-a842-b55aa59b347c",
   "metadata": {},
   "source": [
    "Importing necessary libraries\n",
    "sklearn - used for preprocessing of data\n",
    "torch - pytorch for neural networks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ce4c88-bb02-4f63-a19c-ae31db45117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.read_csv(\"diabetes.csv\") #loading the dataset using pandas , so ds is a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba01f3ac-4e4c-491b-a334-2dd4a7cdc0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds.iloc[:,0:-1].values\n",
    "y = ds.iloc[: , -1].values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "706b5305-3244-428e-acdd-abc663c21c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a162a9e-0b66-411b-ad76-b823fd6009f2",
   "metadata": {},
   "source": [
    "Dividing the feature and label columns \n",
    "since we use .values at the end now X and y are numpy arrays , if u dont use .values they will stay as pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5227b1e2-9665-4541-a401-71876208e730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'negative' 'positive' 'negative' 'positive' 'negative'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'positive' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'positive' 'negative'\n",
      " 'negative' 'negative' 'negative' 'positive' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'positive' 'positive' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'negative' 'negative'\n",
      " 'positive' 'positive' 'positive' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'positive' 'positive'\n",
      " 'negative' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'negative' 'positive' 'positive'\n",
      " 'negative' 'negative' 'negative' 'positive' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'positive' 'negative' 'negative'\n",
      " 'positive' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'positive' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'positive' 'positive' 'positive'\n",
      " 'positive' 'negative' 'positive' 'positive' 'positive' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'positive' 'negative' 'negative'\n",
      " 'positive' 'negative' 'positive' 'negative' 'negative' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'positive' 'negative'\n",
      " 'negative' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'positive' 'positive' 'positive'\n",
      " 'negative' 'negative' 'positive' 'negative' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'positive' 'negative' 'negative'\n",
      " 'positive' 'negative' 'positive' 'positive' 'negative' 'negative'\n",
      " 'positive' 'negative' 'positive' 'negative' 'negative' 'positive'\n",
      " 'negative' 'positive' 'negative' 'positive' 'positive' 'positive'\n",
      " 'negative' 'negative' 'positive' 'negative' 'positive' 'negative'\n",
      " 'negative' 'negative' 'positive' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'positive' 'positive' 'negative' 'positive'\n",
      " 'positive' 'negative' 'negative' 'positive' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'positive' 'positive' 'negative'\n",
      " 'negative' 'negative' 'negative' 'positive' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'positive' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'positive' 'negative'\n",
      " 'negative' 'positive' 'negative' 'positive' 'positive' 'negative'\n",
      " 'positive' 'negative' 'positive' 'negative' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'positive' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'positive' 'positive'\n",
      " 'negative' 'positive' 'negative' 'positive' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'positive' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'negative' 'negative' 'positive'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'positive' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'positive' 'negative'\n",
      " 'negative' 'negative' 'positive' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'negative' 'positive'\n",
      " 'positive' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'positive' 'positive' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'positive' 'positive' 'negative'\n",
      " 'negative' 'negative' 'positive' 'negative' 'positive' 'negative'\n",
      " 'positive' 'negative' 'positive' 'negative' 'positive' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'positive' 'negative'\n",
      " 'negative' 'negative' 'negative' 'positive' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'positive' 'negative' 'positive' 'negative' 'negative' 'negative'\n",
      " 'positive' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'negative' 'negative' 'negative'\n",
      " 'positive' 'negative' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'positive' 'positive' 'positive' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'positive' 'positive' 'negative'\n",
      " 'positive' 'negative' 'negative' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'negative' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'positive' 'negative'\n",
      " 'positive' 'positive' 'negative' 'negative' 'positive' 'negative'\n",
      " 'negative' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'negative' 'negative' 'negative'\n",
      " 'negative' 'negative' 'negative' 'negative' 'positive' 'positive'\n",
      " 'positive' 'negative' 'negative' 'negative' 'negative' 'negative'\n",
      " 'negative' 'positive' 'positive' 'negative' 'negative' 'positive'\n",
      " 'negative' 'negative' 'positive' 'negative' 'positive' 'positive'\n",
      " 'positive' 'negative' 'negative' 'positive' 'positive' 'positive'\n",
      " 'negative' 'positive' 'negative' 'positive' 'negative' 'positive'\n",
      " 'negative' 'negative' 'negative' 'negative' 'positive' 'negative']\n",
      "[1 0 1 0 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 0 1 0 1 0\n",
      " 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1\n",
      " 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0\n",
      " 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 1 0 1 1 1 1\n",
      " 0 0 0 0 0 1 0 0 1 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      " 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 1 0 0 1 0 0 0 1 1 1 0 0\n",
      " 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 1 0 1 1 1 0 0 1 0 1 0 0 0 1\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 1 0 0 1 0 0 1 0 0 1\n",
      " 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 1 0 0 1 0 1 1 0 1 0 1 0 1\n",
      " 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 1\n",
      " 1 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0\n",
      " 1 0 0 1 0 0 1 0 0 0 0 1 1 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 1 1 1 0\n",
      " 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 0 1 1\n",
      " 0 0 0 1 0 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1\n",
      " 1 0 0 1 0 0 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "print(y)\n",
    "y = le.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a3b45aba-b69c-4246-b3b5-71d4c828cb1f",
   "metadata": {},
   "source": [
    "Since our neural network can only work with numerical \n",
    "we change positive to 1 and negative to 0 using \"label Encoder\" from sklearn, we can do it manually using a \"for\" loop too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619e3a29-eff2-43dc-93c0-ae23a677958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y, dtype = 'float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c490c94-987d-4db0-b597-0c26a0e8bc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29a3e104-94df-4c31-8d59-03dbd1f994a4",
   "metadata": {},
   "source": [
    "Since our features have different ranges of numbers, the neural network would be biased focusing on only few features, to avoid this we \n",
    "normalize all the features to (-3,3) using standardScaler from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59b9d6a6-ebca-4c1f-ac9d-b6417d827692",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(X)\n",
    "y = torch.tensor(y)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "160a25f0-2260-4140-b850-bc916481efa2",
   "metadata": {},
   "source": [
    "Converting np arrays to pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd6a560-2c80-4dfe-ab57-47fb915f115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aac04a13-bdc5-4a9a-b888-6ec746a14af6",
   "metadata": {},
   "source": [
    "Unsqeezing is a technique of adding dimension in pytorch \n",
    "It is done to avoid in consistencies while calculating loss and backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1fc6b2f-8338-4400-a8f0-dc4a47d60c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index] , self.y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0d018d8-c5f1-460b-a2d2-0c85b49dfec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.6399,  0.8483,  0.1496,  0.9073, -0.6929,  0.2040,  1.4260],\n",
      "       dtype=torch.float64), tensor([1.], dtype=torch.float64))\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(X,y)\n",
    "len(dataset)\n",
    "print(dataset[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59840c1b-859e-4800-9afd-2598c5df0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(dataset = dataset , batch_size = 32 , shuffle = True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3409bd4b-5332-4eeb-8a9d-b8e048437463",
   "metadata": {},
   "source": [
    "using builtin dataset class from pytorch and overriding some of its methods \n",
    "this is used for better loading and performing of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fffbe895-e8e9-43d8-ad28-de0642549b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 24 batches in the dataset\n",
      "For one iteration (batch), there is:\n",
      "Data:    torch.Size([32, 7])\n",
      "Labels:  torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "print(\"There is {} batches in the dataset\".format(len(trainLoader)))\n",
    "for (x,y) in trainLoader:\n",
    "    print(\"For one iteration (batch), there is:\")\n",
    "    print(\"Data:    {}\".format(x.shape))\n",
    "    print(\"Labels:  {}\".format(y.shape))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "35b58bb4-1633-4498-b656-176065bf7a3c",
   "metadata": {},
   "source": [
    "the above code helps us in visualizing the mini batches  , just for understanding purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "04d0a4c5-ddc8-47da-af44-53d231603288",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, inputFeatures , outputFeatures):\n",
    "        super(Model , self).__init__()\n",
    "        self.fc1 = nn.Linear(inputFeatures , 5)\n",
    "        self.fc2 = nn.Linear(5 , 4)\n",
    "        self.fc3 = nn.Linear(4 , 3)\n",
    "        self.fc4 = nn.Linear(3 , outputFeatures)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "    def forward(self,X):\n",
    "        out = self.fc1(X)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.tanh(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b89dafb-0902-4edb-b92c-8ef028e051a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\anaconda3\\Lib\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "net = Model(7,1)\n",
    "criterion = torch.nn.BCELoss(size_average  = True)\n",
    "optimizer = torch.optim.SGD(net.parameters() , lr = 0.1 , momentum = 0.9 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfcbd2ad-97fa-4420-801d-4c458e1622a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss : 0.593 , Accuracy : 0.688\n",
      "Epoch 2/200, Loss : 0.333 , Accuracy : 0.844\n",
      "Epoch 3/200, Loss : 0.555 , Accuracy : 0.688\n",
      "Epoch 4/200, Loss : 0.383 , Accuracy : 0.812\n",
      "Epoch 5/200, Loss : 0.449 , Accuracy : 0.781\n",
      "Epoch 6/200, Loss : 0.868 , Accuracy : 0.531\n",
      "Epoch 7/200, Loss : 0.553 , Accuracy : 0.656\n",
      "Epoch 8/200, Loss : 0.393 , Accuracy : 0.812\n",
      "Epoch 9/200, Loss : 0.496 , Accuracy : 0.656\n",
      "Epoch 10/200, Loss : 0.400 , Accuracy : 0.781\n",
      "Epoch 11/200, Loss : 0.500 , Accuracy : 0.719\n",
      "Epoch 12/200, Loss : 0.423 , Accuracy : 0.781\n",
      "Epoch 13/200, Loss : 0.587 , Accuracy : 0.719\n",
      "Epoch 14/200, Loss : 0.684 , Accuracy : 0.688\n",
      "Epoch 15/200, Loss : 0.356 , Accuracy : 0.781\n",
      "Epoch 16/200, Loss : 0.554 , Accuracy : 0.750\n",
      "Epoch 17/200, Loss : 0.470 , Accuracy : 0.844\n",
      "Epoch 18/200, Loss : 0.498 , Accuracy : 0.750\n",
      "Epoch 19/200, Loss : 0.389 , Accuracy : 0.781\n",
      "Epoch 20/200, Loss : 0.476 , Accuracy : 0.812\n",
      "Epoch 21/200, Loss : 0.503 , Accuracy : 0.688\n",
      "Epoch 22/200, Loss : 0.509 , Accuracy : 0.750\n",
      "Epoch 23/200, Loss : 0.310 , Accuracy : 0.875\n",
      "Epoch 24/200, Loss : 0.556 , Accuracy : 0.750\n",
      "Epoch 25/200, Loss : 0.496 , Accuracy : 0.719\n",
      "Epoch 26/200, Loss : 0.470 , Accuracy : 0.750\n",
      "Epoch 27/200, Loss : 0.430 , Accuracy : 0.750\n",
      "Epoch 28/200, Loss : 0.382 , Accuracy : 0.781\n",
      "Epoch 29/200, Loss : 0.530 , Accuracy : 0.844\n",
      "Epoch 30/200, Loss : 0.407 , Accuracy : 0.844\n",
      "Epoch 31/200, Loss : 0.479 , Accuracy : 0.750\n",
      "Epoch 32/200, Loss : 0.313 , Accuracy : 0.906\n",
      "Epoch 33/200, Loss : 0.591 , Accuracy : 0.719\n",
      "Epoch 34/200, Loss : 0.600 , Accuracy : 0.719\n",
      "Epoch 35/200, Loss : 0.556 , Accuracy : 0.750\n",
      "Epoch 36/200, Loss : 0.318 , Accuracy : 0.875\n",
      "Epoch 37/200, Loss : 0.500 , Accuracy : 0.719\n",
      "Epoch 38/200, Loss : 0.667 , Accuracy : 0.719\n",
      "Epoch 39/200, Loss : 0.401 , Accuracy : 0.750\n",
      "Epoch 40/200, Loss : 0.365 , Accuracy : 0.844\n",
      "Epoch 41/200, Loss : 0.507 , Accuracy : 0.719\n",
      "Epoch 42/200, Loss : 0.635 , Accuracy : 0.750\n",
      "Epoch 43/200, Loss : 0.634 , Accuracy : 0.688\n",
      "Epoch 44/200, Loss : 0.472 , Accuracy : 0.844\n",
      "Epoch 45/200, Loss : 0.346 , Accuracy : 0.844\n",
      "Epoch 46/200, Loss : 0.521 , Accuracy : 0.781\n",
      "Epoch 47/200, Loss : 0.335 , Accuracy : 0.844\n",
      "Epoch 48/200, Loss : 0.445 , Accuracy : 0.781\n",
      "Epoch 49/200, Loss : 0.414 , Accuracy : 0.844\n",
      "Epoch 50/200, Loss : 0.340 , Accuracy : 0.875\n",
      "Epoch 51/200, Loss : 0.508 , Accuracy : 0.781\n",
      "Epoch 52/200, Loss : 0.293 , Accuracy : 0.906\n",
      "Epoch 53/200, Loss : 0.370 , Accuracy : 0.875\n",
      "Epoch 54/200, Loss : 0.498 , Accuracy : 0.781\n",
      "Epoch 55/200, Loss : 0.313 , Accuracy : 0.906\n",
      "Epoch 56/200, Loss : 0.225 , Accuracy : 0.875\n",
      "Epoch 57/200, Loss : 0.297 , Accuracy : 0.844\n",
      "Epoch 58/200, Loss : 0.374 , Accuracy : 0.875\n",
      "Epoch 59/200, Loss : 0.300 , Accuracy : 0.875\n",
      "Epoch 60/200, Loss : 0.490 , Accuracy : 0.719\n",
      "Epoch 61/200, Loss : 0.319 , Accuracy : 0.875\n",
      "Epoch 62/200, Loss : 0.468 , Accuracy : 0.781\n",
      "Epoch 63/200, Loss : 0.435 , Accuracy : 0.812\n",
      "Epoch 64/200, Loss : 0.541 , Accuracy : 0.781\n",
      "Epoch 65/200, Loss : 0.306 , Accuracy : 0.844\n",
      "Epoch 66/200, Loss : 0.314 , Accuracy : 0.875\n",
      "Epoch 67/200, Loss : 0.352 , Accuracy : 0.906\n",
      "Epoch 68/200, Loss : 0.480 , Accuracy : 0.719\n",
      "Epoch 69/200, Loss : 0.232 , Accuracy : 0.906\n",
      "Epoch 70/200, Loss : 0.480 , Accuracy : 0.812\n",
      "Epoch 71/200, Loss : 0.397 , Accuracy : 0.812\n",
      "Epoch 72/200, Loss : 0.422 , Accuracy : 0.844\n",
      "Epoch 73/200, Loss : 0.316 , Accuracy : 0.875\n",
      "Epoch 74/200, Loss : 0.400 , Accuracy : 0.781\n",
      "Epoch 75/200, Loss : 0.340 , Accuracy : 0.844\n",
      "Epoch 76/200, Loss : 0.505 , Accuracy : 0.781\n",
      "Epoch 77/200, Loss : 0.268 , Accuracy : 0.875\n",
      "Epoch 78/200, Loss : 0.300 , Accuracy : 0.906\n",
      "Epoch 79/200, Loss : 0.509 , Accuracy : 0.750\n",
      "Epoch 80/200, Loss : 0.374 , Accuracy : 0.844\n",
      "Epoch 81/200, Loss : 0.341 , Accuracy : 0.875\n",
      "Epoch 82/200, Loss : 0.473 , Accuracy : 0.812\n",
      "Epoch 83/200, Loss : 0.361 , Accuracy : 0.844\n",
      "Epoch 84/200, Loss : 0.374 , Accuracy : 0.812\n",
      "Epoch 85/200, Loss : 0.391 , Accuracy : 0.812\n",
      "Epoch 86/200, Loss : 0.230 , Accuracy : 0.906\n",
      "Epoch 87/200, Loss : 0.429 , Accuracy : 0.750\n",
      "Epoch 88/200, Loss : 0.375 , Accuracy : 0.812\n",
      "Epoch 89/200, Loss : 0.286 , Accuracy : 0.875\n",
      "Epoch 90/200, Loss : 0.528 , Accuracy : 0.688\n",
      "Epoch 91/200, Loss : 0.518 , Accuracy : 0.750\n",
      "Epoch 92/200, Loss : 0.393 , Accuracy : 0.875\n",
      "Epoch 93/200, Loss : 0.315 , Accuracy : 0.875\n",
      "Epoch 94/200, Loss : 0.547 , Accuracy : 0.719\n",
      "Epoch 95/200, Loss : 0.530 , Accuracy : 0.750\n",
      "Epoch 96/200, Loss : 0.325 , Accuracy : 0.875\n",
      "Epoch 97/200, Loss : 0.531 , Accuracy : 0.656\n",
      "Epoch 98/200, Loss : 0.414 , Accuracy : 0.781\n",
      "Epoch 99/200, Loss : 0.301 , Accuracy : 0.906\n",
      "Epoch 100/200, Loss : 0.554 , Accuracy : 0.750\n",
      "Epoch 101/200, Loss : 0.565 , Accuracy : 0.688\n",
      "Epoch 102/200, Loss : 0.428 , Accuracy : 0.844\n",
      "Epoch 103/200, Loss : 0.378 , Accuracy : 0.812\n",
      "Epoch 104/200, Loss : 0.366 , Accuracy : 0.844\n",
      "Epoch 105/200, Loss : 0.601 , Accuracy : 0.781\n",
      "Epoch 106/200, Loss : 0.454 , Accuracy : 0.781\n",
      "Epoch 107/200, Loss : 0.355 , Accuracy : 0.812\n",
      "Epoch 108/200, Loss : 0.517 , Accuracy : 0.750\n",
      "Epoch 109/200, Loss : 0.598 , Accuracy : 0.688\n",
      "Epoch 110/200, Loss : 0.481 , Accuracy : 0.719\n",
      "Epoch 111/200, Loss : 0.308 , Accuracy : 0.875\n",
      "Epoch 112/200, Loss : 0.523 , Accuracy : 0.750\n",
      "Epoch 113/200, Loss : 0.362 , Accuracy : 0.875\n",
      "Epoch 114/200, Loss : 0.561 , Accuracy : 0.688\n",
      "Epoch 115/200, Loss : 0.222 , Accuracy : 0.906\n",
      "Epoch 116/200, Loss : 0.330 , Accuracy : 0.906\n",
      "Epoch 117/200, Loss : 0.245 , Accuracy : 0.938\n",
      "Epoch 118/200, Loss : 0.505 , Accuracy : 0.719\n",
      "Epoch 119/200, Loss : 0.408 , Accuracy : 0.781\n",
      "Epoch 120/200, Loss : 0.390 , Accuracy : 0.781\n",
      "Epoch 121/200, Loss : 0.371 , Accuracy : 0.844\n",
      "Epoch 122/200, Loss : 0.398 , Accuracy : 0.844\n",
      "Epoch 123/200, Loss : 0.527 , Accuracy : 0.688\n",
      "Epoch 124/200, Loss : 0.354 , Accuracy : 0.875\n",
      "Epoch 125/200, Loss : 0.500 , Accuracy : 0.750\n",
      "Epoch 126/200, Loss : 0.351 , Accuracy : 0.875\n",
      "Epoch 127/200, Loss : 0.499 , Accuracy : 0.844\n",
      "Epoch 128/200, Loss : 0.313 , Accuracy : 0.938\n",
      "Epoch 129/200, Loss : 0.460 , Accuracy : 0.844\n",
      "Epoch 130/200, Loss : 0.318 , Accuracy : 0.875\n",
      "Epoch 131/200, Loss : 0.426 , Accuracy : 0.812\n",
      "Epoch 132/200, Loss : 0.394 , Accuracy : 0.844\n",
      "Epoch 133/200, Loss : 0.373 , Accuracy : 0.844\n",
      "Epoch 134/200, Loss : 0.588 , Accuracy : 0.750\n",
      "Epoch 135/200, Loss : 0.404 , Accuracy : 0.875\n",
      "Epoch 136/200, Loss : 0.399 , Accuracy : 0.844\n",
      "Epoch 137/200, Loss : 0.457 , Accuracy : 0.844\n",
      "Epoch 138/200, Loss : 0.431 , Accuracy : 0.812\n",
      "Epoch 139/200, Loss : 0.499 , Accuracy : 0.688\n",
      "Epoch 140/200, Loss : 0.392 , Accuracy : 0.844\n",
      "Epoch 141/200, Loss : 0.289 , Accuracy : 0.906\n",
      "Epoch 142/200, Loss : 0.270 , Accuracy : 0.938\n",
      "Epoch 143/200, Loss : 0.430 , Accuracy : 0.781\n",
      "Epoch 144/200, Loss : 0.580 , Accuracy : 0.719\n",
      "Epoch 145/200, Loss : 0.338 , Accuracy : 0.875\n",
      "Epoch 146/200, Loss : 0.505 , Accuracy : 0.750\n",
      "Epoch 147/200, Loss : 0.338 , Accuracy : 0.844\n",
      "Epoch 148/200, Loss : 0.285 , Accuracy : 0.875\n",
      "Epoch 149/200, Loss : 0.539 , Accuracy : 0.750\n",
      "Epoch 150/200, Loss : 0.479 , Accuracy : 0.781\n",
      "Epoch 151/200, Loss : 0.254 , Accuracy : 0.906\n",
      "Epoch 152/200, Loss : 0.522 , Accuracy : 0.750\n",
      "Epoch 153/200, Loss : 0.358 , Accuracy : 0.906\n",
      "Epoch 154/200, Loss : 0.297 , Accuracy : 0.969\n",
      "Epoch 155/200, Loss : 0.410 , Accuracy : 0.875\n",
      "Epoch 156/200, Loss : 0.499 , Accuracy : 0.750\n",
      "Epoch 157/200, Loss : 0.491 , Accuracy : 0.781\n",
      "Epoch 158/200, Loss : 0.351 , Accuracy : 0.781\n",
      "Epoch 159/200, Loss : 0.298 , Accuracy : 0.844\n",
      "Epoch 160/200, Loss : 0.413 , Accuracy : 0.781\n",
      "Epoch 161/200, Loss : 0.533 , Accuracy : 0.688\n",
      "Epoch 162/200, Loss : 0.319 , Accuracy : 0.812\n",
      "Epoch 163/200, Loss : 0.471 , Accuracy : 0.781\n",
      "Epoch 164/200, Loss : 0.516 , Accuracy : 0.781\n",
      "Epoch 165/200, Loss : 0.393 , Accuracy : 0.812\n",
      "Epoch 166/200, Loss : 0.431 , Accuracy : 0.812\n",
      "Epoch 167/200, Loss : 0.466 , Accuracy : 0.844\n",
      "Epoch 168/200, Loss : 0.603 , Accuracy : 0.719\n",
      "Epoch 169/200, Loss : 0.313 , Accuracy : 0.906\n",
      "Epoch 170/200, Loss : 0.385 , Accuracy : 0.812\n",
      "Epoch 171/200, Loss : 0.346 , Accuracy : 0.875\n",
      "Epoch 172/200, Loss : 0.338 , Accuracy : 0.875\n",
      "Epoch 173/200, Loss : 0.383 , Accuracy : 0.844\n",
      "Epoch 174/200, Loss : 0.366 , Accuracy : 0.812\n",
      "Epoch 175/200, Loss : 0.364 , Accuracy : 0.812\n",
      "Epoch 176/200, Loss : 0.314 , Accuracy : 0.875\n",
      "Epoch 177/200, Loss : 0.552 , Accuracy : 0.719\n",
      "Epoch 178/200, Loss : 0.362 , Accuracy : 0.875\n",
      "Epoch 179/200, Loss : 0.399 , Accuracy : 0.781\n",
      "Epoch 180/200, Loss : 0.504 , Accuracy : 0.750\n",
      "Epoch 181/200, Loss : 0.443 , Accuracy : 0.781\n",
      "Epoch 182/200, Loss : 0.546 , Accuracy : 0.750\n",
      "Epoch 183/200, Loss : 0.318 , Accuracy : 0.844\n",
      "Epoch 184/200, Loss : 0.414 , Accuracy : 0.844\n",
      "Epoch 185/200, Loss : 0.396 , Accuracy : 0.844\n",
      "Epoch 186/200, Loss : 0.381 , Accuracy : 0.812\n",
      "Epoch 187/200, Loss : 0.260 , Accuracy : 0.875\n",
      "Epoch 188/200, Loss : 0.336 , Accuracy : 0.844\n",
      "Epoch 189/200, Loss : 0.472 , Accuracy : 0.750\n",
      "Epoch 190/200, Loss : 0.766 , Accuracy : 0.656\n",
      "Epoch 191/200, Loss : 0.315 , Accuracy : 0.875\n",
      "Epoch 192/200, Loss : 0.350 , Accuracy : 0.844\n",
      "Epoch 193/200, Loss : 0.608 , Accuracy : 0.750\n",
      "Epoch 194/200, Loss : 0.356 , Accuracy : 0.844\n",
      "Epoch 195/200, Loss : 0.439 , Accuracy : 0.781\n",
      "Epoch 196/200, Loss : 0.608 , Accuracy : 0.688\n",
      "Epoch 197/200, Loss : 0.366 , Accuracy : 0.781\n",
      "Epoch 198/200, Loss : 0.319 , Accuracy : 0.812\n",
      "Epoch 199/200, Loss : 0.358 , Accuracy : 0.844\n",
      "Epoch 200/200, Loss : 0.409 , Accuracy : 0.844\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for epoch in range(200):\n",
    "    for inputs,labels in trainLoader:\n",
    "        inputs = inputs.float()\n",
    "        labels = labels.float()\n",
    "        outputs = net(inputs) #this is forward prop\n",
    "        loss = criterion(outputs , labels)\n",
    "        optimizer.zero_grad() # without causing accumulation it sets to zero\n",
    "        loss.backward() #computes back prop gradients \n",
    "        optimizer.step() #updates grapdients here \n",
    "    outputs = (outputs > 0.5).float()\n",
    "    accuracy = (outputs == labels).float().mean()\n",
    "    print(\"Epoch {}/{}, Loss : {:.3f} , Accuracy : {:.3f}\".format(epoch +1 , epochs , loss , accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df88e2f-7a2c-465c-ae94-a4152dafd4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
